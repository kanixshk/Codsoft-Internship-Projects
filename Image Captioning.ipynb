{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99009d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16  # Or ResNet50\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d96fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ee3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = 'Cars.zip'\n",
    "extract_path = '/content/images'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "image_dir = extract_path\n",
    "print(\"Images extracted to:\", image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f991306b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.vgg16.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "image_files = [os.path.join(extract_path, f) for f in os.listdir(extract_path) if f.lower().endswith('.jpg')]\n",
    "\n",
    "preprocessed_images = [preprocess_image(path) for path in image_files]\n",
    "\n",
    "print(f\"Processed {len(preprocessed_images)} images.\")\n",
    "\n",
    "cnn_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "image_features = {}\n",
    "for img_path in image_files:\n",
    "    features = cnn_model.predict(preprocess_image(img_path))\n",
    "    image_features[img_path] = features.flatten()\n",
    "print(\"Extracted features from images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810e5fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c866a0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_file = 'Captions.txt'caption_file = 'Captions.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f673af",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_captions = {}\n",
    "with open(caption_file, 'r') as f:\n",
    "    for line in f:\n",
    "        image_path, caption = line.strip().split(' ', 1)\n",
    "        if image_path not in image_captions:\n",
    "            image_captions[image_path] = []\n",
    "        image_captions[image_path].append(caption)\n",
    "\n",
    "all_captions = [caption for captions in image_captions.values() for caption in captions]\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "caption_sequences = {}\n",
    "for img_path, captions in image_captions.items():\n",
    "    sequences = tokenizer.texts_to_sequences(captions)\n",
    "    caption_sequences[img_path] = sequences\n",
    "\n",
    "max_caption_length = max(len(seq) for seqs in caption_sequences.values() for seq in seqs)\n",
    "padded_sequences = {}\n",
    "for img_path, sequences in caption_sequences.items():\n",
    "    padded_sequences[img_path] = pad_sequences(sequences, maxlen=max_caption_length, padding='post')\n",
    "\n",
    "\n",
    "\n",
    "captions_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e586da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(caption_file, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        if len(parts) == 2:\n",
    "            image_path, caption = parts\n",
    "            captions_dict[image_path] = caption\n",
    "        else:\n",
    "            print(\"Skipping malformed line:\", line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc048d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = Input(shape=(cnn_model.output_shape[1],))\n",
    "\n",
    "caption_input = Input(shape=(max_caption_length,))\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, 256, mask_zero=True)(caption_input)\n",
    "\n",
    "rnn_layer = LSTM(256)(embedding_layer)\n",
    "\n",
    "decoder = tf.keras.layers.concatenate([image_input, rnn_layer])\n",
    "\n",
    "output = Dense(vocab_size, activation='softmax')(decoder)\n",
    "\n",
    "model = Model(inputs=[image_input, caption_input], outputs=output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86397f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, img_path in enumerate(image_files):\n",
    "    file_name = os.path.basename(img_path)\n",
    "    caption = captions_dict.get(file_name, \"No caption found\")\n",
    "    preprocessed = preprocess_image(os.path.join(extract_path, file_name))\n",
    "    print(f\"{file_name}: {caption} | Preprocessed shape: {preprocessed.shape}\")\n",
    "    if i >= 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8018b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Preprocessing and Feature Extraction Results ---\")\n",
    "if preprocessed_images:\n",
    "  print(\"Shape of the first preprocessed image:\", preprocessed_images[0].shape)\n",
    "else:\n",
    "  print(\"No images were preprocessed.\")\n",
    "\n",
    "print(\"\\nFirst 2 entries of image_features:\")\n",
    "for i, (key, value) in enumerate(image_features.items()):\n",
    "  print(f\"{key}: shape {value.shape}\")\n",
    "  if i >= 1:\n",
    "    break\n",
    "\n",
    "print(\"\\nFirst 2 entries of image_captions:\")\n",
    "for i, (key, value) in enumerate(image_captions.items()):\n",
    "  print(f\"{key}: {value}\")\n",
    "  if i >= 1:\n",
    "    break\n",
    "\n",
    "print(f\"\\nVocabulary size: {vocab_size}\")\n",
    "print(f\"Maximum caption length: {max_caption_length}\")\n",
    "print(\"\\nFirst 10 words in tokenizer's word index:\")\n",
    "# We can print the first few items of the word index to see the vocabulary\n",
    "for i, (word, index) in enumerate(tokenizer.word_index.items()):\n",
    "  print(f\"{word}: {index}\")\n",
    "  if i >= 9:\n",
    "    break\n",
    "\n",
    "print(\"\\nShape of padded sequences for the first image:\")\n",
    "if list(padded_sequences.values()):\n",
    "    print(list(padded_sequences.values())[0].shape)\n",
    "else:\n",
    "    print(\"No padded sequences generated.\")\n",
    "\n",
    "print(\"\\n--- Model Summary ---\")\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
